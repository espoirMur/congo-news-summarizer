{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm.generator import LLamaCppGeneratorComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are a french news reporter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv ./.env_local -o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"http://149.102.141.96\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cpp_generator = LLamaCppGeneratorComponent(\n",
    "    api_url=API_URL,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cpp_generator._ping_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"news-clusters-2024-12-03-to-2024-12-03.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"congonews-clusters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader as csv_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.shared.cloud_storage.cloud_storage_non_numpy import BackBlazeCloudStorageCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_storage = BackBlazeCloudStorageCSV(environment=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm.generator import SUMMARIZATION_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "def sort_function(x): return x[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cloud_storage.read_file_as_list(\n",
    "    bucket_name=BUCKET_NAME, file_name=filename)\n",
    "\n",
    "sorted_data = sorted(data, key=sort_function)\n",
    "grouped_data = groupby(sorted_data, key=sort_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input = llama_cpp_generator.generate_chat_input(\n",
    "    template_values={\"content\": content}, prompt_template=SUMMARIZATION_PROMPT_TEMPLATE)\n",
    "chat_tokens = llama_cpp_generator.apply_chat_template(\n",
    "            messages=chat_input, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "data = {\n",
    "        \"prompt\": chat_tokens,\n",
    "        \"n_predict\": 512,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_k\": 40,\n",
    "        \"top_p\": 0.90,\n",
    "        \"stopped_eos\": True,\n",
    "        \"repeat_penalty\": 1.05,\n",
    "        \"stop\": [\n",
    "            \"assistant\",\n",
    "            \"<|im_end|>\",\n",
    "        ],\n",
    "        \"seed\": 42,\n",
    "    }\n",
    "json_data = json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"{llama_cpp_generator.api_url}/completion\",\n",
    "    headers=headers,\n",
    "    data=json_data,\n",
    "    timeout=300,\n",
    "    auth=llama_cpp_generator.auth_header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cpp_generator._ping_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    This function generates response using the Llamma.cpp api\n",
    "    Args:\n",
    "        prompt (str): The prompt to generate response from\n",
    "    Returns:\n",
    "        str: The generated response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Send the POST request\n",
    "    try:\n",
    "        \n",
    "        return response.json()[\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llama_cpp_generator.generate_response(chat_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, group in grouped_data:\n",
    "    news_data = list(group)\n",
    "    titles = [news[\"title\"] for news in news_data]\n",
    "    urls = [news[\"url\"] for news in news_data]\n",
    "    content = \"\\n\".join([news[\"content\"] for news in news_data])\n",
    "    content = normalize(\"NFKD\", content)\n",
    "    summary = llama_cpp_generator.run(\n",
    "        template_values={\"content\": content}, prompt_template=SUMMARIZATION_PROMPT_TEMPLATE)\n",
    "    news_data = {\n",
    "        \"titles\": titles,\n",
    "        \"urls\": urls,\n",
    "        \"summary\": summary\n",
    "    }\n",
    "    summaries.append(news_data)\n",
    "    print(f\" This is the summary for label {label}\")\n",
    "    pprint(summary)\n",
    "    print(f\"Here are the titles for label {label}\")\n",
    "    print(10 * \"*\")\n",
    "    for title in titles:\n",
    "        print(title)\n",
    "    print(10 * \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will keep both model in the server and use them. I will be benchmackring them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat template without huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news_summaries.json\", \"w\") as f:\n",
    "    json.dump(summaries, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries\n",
    "for summary in summaries:\n",
    "    pprint(summary[\"summary\"])\n",
    "    for title_ in summary[\"titles\"]:\n",
    "        print(title_)\n",
    "    print(10 * \"*\")\n",
    "\n",
    "    print(10 * \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

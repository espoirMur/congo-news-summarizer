{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e5f93-6606-489f-9464-87a28e83013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad95e2c-cebb-4965-9062-4d7de97328a0",
   "metadata": {},
   "source": [
    "## Congo News Summarizer. Part one: News Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a827ba-aa76-4a85-8098-dcbe67ba6410",
   "metadata": {},
   "source": [
    "Over the past months, I have been collecting a lot of news articles from major Congolese news websites. I have those articles saved in a Postgres database. There is a lot of fun stuff I can do with them. Among them is a news summarizer. I want to analyze the daily news and find out what the websites are talking about.\n",
    "\n",
    "In this blog or series of posts, I will try to build that news summarizer.\n",
    "\n",
    "In the first part, I will talk about how I built the news clustering model, Then in the second part, I will talk about how I built an LLM that summarizes each news cluster and how I deployed it. Finally, in the last part, I will talk about how to scale the model and deploy it in a production setting.\n",
    "\n",
    "The end goal of this project is two folds. First, I want to have a news summarizer that I can open a morning and it will give me a summary of major news that is happening in Congo. Second, while building this news summarizer I would like to sharpen my ML Engineering Knowledge and illustrate that I can build an end-to-end production-ready project with the latest Python stack.\n",
    "\n",
    "To start, let us talk about how I build the news clustering model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f24ba8-f0d8-4e7a-8abe-d453bd503fba",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "### The Data\n",
    "\n",
    "I have scrappers that run every day and scrape the data that Congolese news websites produce, those articles are saved as text in a Postgres database.\n",
    "\n",
    "I will query that database and load the data in the pandas dataframe for better analysis. I have the code to connect and read from the Postgres database embedded in modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c3fea-c548-4030-b857-7db9366e8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a97c90-20f7-496b-be2e-b024cd270793",
   "metadata": {},
   "outputs": [],
   "source": [
    "%dotenv ./.env_prod -o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48bf007-62e8-4532-b4c4-31bbf16b81fa",
   "metadata": {},
   "source": [
    "The above line loads the database credentials so that we can query the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c6ef8-470e-42f1-ab8d-d0b987abf83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.shared.database import execute_query, generate_database_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d0781-d8da-4ea7-b3f8-ca9041392298",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_date = \"2024-11-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db7bed-b387-4d77-95e0-de2ad43145d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday_article_query = f\"select content, title, posted_at,url from article where posted_at::date = '{experiment_date}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ca245-70d6-4883-b669-aa2db8886ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday_article_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1db0ab-e09f-4902-a754-878071f2316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684204f3-df24-49dc-b0df-3e315b416269",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_user = getenv('POSTGRES_USER')\n",
    "database_password = getenv('POSTGRES_PASSWORD')\n",
    "database_host = getenv('POSTGRES_HOST')\n",
    "database_port = getenv('POSTGRES_PORT')\n",
    "database_name = getenv('POSTGRES_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee77219-df10-4432-a131-9d2ee19e8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_credentials = {\n",
    "    'user': database_user,\n",
    "    'password': database_password,\n",
    "    'host': database_host,\n",
    "    'port': database_port,\n",
    "    'database': database_name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30c8ed-3e97-4f3c-9653-c696a18dfca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = generate_database_connection(\n",
    "    database_credential=database_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e31740-d68a-45c6-9952-17dee6eb2278",
   "metadata": {},
   "source": [
    "With the credentials, the database connection, the query we can go ahead and query the database to retrieve the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251cc08d-4b84-457e-80e6-899b0df0d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = execute_query(query=yesterday_article_query,\n",
    "                        database_connection=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a169c42-0cdc-43b9-82a7-fe4dcd9c051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b5fa4-a8c7-4f4c-93de-d753613d6387",
   "metadata": {},
   "source": [
    "We have our results in a list now we can put them in a dataframe from further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4230a-79e2-4cf7-9a1d-a0f18b26991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a144f2b-14fd-4878-ac2f-ee1a4420f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e321c50-045f-4a62-9d11-2207288dc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb08e3-023c-4665-9f0c-e943692faaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.columns = [\"content\", \"title\", \"posted_at\", \"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11157fd-6a83-43db-a22f-3201f7135c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b691e-2573-4ad2-8777-7555112abfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = Path.cwd().parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69c2ce-2dad-48c1-8b84-abd8861c07b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf6534-6f15-486f-8013-f41b900f2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8265b63-f037-4fd2-980d-a6c185a10d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_directory = current_directory.joinpath(\"datasets\", \"today_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23171b-1972-44d5-9515-cc49448d286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_directory.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736a278-2358-46d8-8ea4-fbf5e25eb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf80033-65fe-4289-b196-d8a1028b8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981211a5-e921-47dd-b7ff-3f9c596d83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv(news_directory.joinpath(f\"{today}-news.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1cc3a-d06a-4b57-85a1-2e251e25e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ccf76",
   "metadata": {},
   "source": [
    "## Reading the data from LocalFile\n",
    "\n",
    "Alternatively to reading the data from the database you can read it form the CSV file at this [path](../data/2024-11-06-news.csv) and load it in a dataframe named `news_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d12f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "current_directory = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = current_directory.joinpath(\"data\", \"2024-11-06-news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(data_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4428962-3e2d-4011-b9d6-58b8467519e6",
   "metadata": {},
   "source": [
    "### Preprocessing.\n",
    "\n",
    "In the above code, we have collected the data for the previous day. For one given day, we can have up to 72 news articles written by different news outlets.\n",
    "\n",
    "We have got our news dataset, we need to now do some preprocessing. The only preprocessing we will do will be to drop the duplicate in the content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5088e74-ec03-49dc-a51e-99ea9c8a09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.drop_duplicates(subset=\"content\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3af6dc-1f5d-400d-8854-67b66926804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9569e5-8b8d-478c-9eaf-2ea62b472d72",
   "metadata": {},
   "source": [
    "## 2. Embedding phase.\n",
    "\n",
    "Machine learning models don't work with text data, we need to convert the text into a representation that computers can understand. To achieve this we need to use embeddings. We will use an embedding model to learn the representation of our dataset in an embedding space.\n",
    "\n",
    "We will be using the `dunzhang/stella_en_400M_v5`, it is a good model from Huggingface, It offers a good trade-off between size and performance. It has a good score on different tasks in both French and English on the [MTEB leaderboard.](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "The below code section assumes that we have the model downloaded in a local repository of our machine. If you want to download the model locally you can refer to this [script](put the path here) to learn how to download the model locally.\n",
    "\n",
    "The code will load the embedding model and use it to encode the news. After the encoding, we will have for each news article an embedding vector of shape 1024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5366596-166a-498b-83bc-3cddccd93d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = \"dunzhang/stella_en_400M_v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91f7fd-6ee4-4d8d-ae5c-4e0863470404",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3d4ae-23d3-41bf-a71f-56e8706a50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = current_directory.joinpath(embedding_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de2515-33c6-4c57-bf56-d6dd9edaa06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_path = current_directory.joinpath(\"models\", embedding_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17da72-e126-4149-8a33-54a204280d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer_kwargs = {\"model_name_or_path\": embedding_model_path.__str__(),\n",
    "                      \"trust_remote_code\": True,\n",
    "                      \"device\": \"cpu\",\n",
    "                      \"config_kwargs\": {\"use_memory_efficient_attention\": False,\n",
    "                                        \"unpad_inputs\": False},\n",
    "                      \"cache_folder\": model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f3124-8d8e-4317-8f8b-2c12e34bda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05843a-b276-4573-98ed-b146c1886ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = SentenceTransformer(\n",
    "    **transformer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac51a9-0c80-4896-bb73-622ced506b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_news_embeddings = sentence_transformer_model.encode(\n",
    "    news_df.content.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804887e-cd95-4b08-a461-73243a132899",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_news_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6afb4-991f-4088-ad41-da1e501deaf5",
   "metadata": {},
   "source": [
    "Now we have encoded our news for each news we have an embedding vector of shape 1024. With those embeddings, we can now start clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8de53d",
   "metadata": {},
   "source": [
    "## 3. Clustering Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d6672-72ff-4bf2-a4ab-1198a47b8777",
   "metadata": {},
   "source": [
    "### Kmeans\n",
    "\n",
    "In this step, we will group our news embeddings in a cluster using the K-mean algorithm. The algorithm will try to group the news in clusters based on the similarity of their embedding vectors. After the clustering, we will have similar news grouped in similar clusters. You can learn more about the clustering algorithm [here](https://www.reddit.com/r/learnmachinelearning/comments/rmx04g/what_is_K-means_clustering_a_2minute_visual_guide/)\n",
    "\n",
    "#### How do we pick the number of clusters?\n",
    "\n",
    "One question that is still unclear in the literature about K-mean is how to pick the number of clusters in the K-mean.\n",
    "A common approach is to use the Silhouette Coefficient. In the next section, I will explain that coefficient.\n",
    "\n",
    "#### Silhouette Score.\n",
    "\n",
    "> The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97ac23-3bb4-43fa-85b6-33afdd0eda20",
   "metadata": {},
   "source": [
    "Given a point $x_i$ that belongs to a cluster labeled $c_i$. To compute the silhouette score:\n",
    "\n",
    "- We compute the mean distance of the $x_i$ to all the other points in cluster $c_i$, we call it $a_i$.\n",
    "\n",
    "  ${\\displaystyle a_i={\\frac {1}{|C_{I}|-1}}\\sum _{j\\in C_{I},i\\neq j}d(i,j)}$\n",
    "\n",
    "  Note that we divide $C_i - 1$ which is the number of points in the cluster minus the current point.\n",
    "\n",
    "- Then we compute $b_i$, which measure how the point $x_i$ in cluster $c_i$ is dissimilar to all other clusters $c_j$ with $c_j != c_i$.\n",
    "\n",
    "For each other clusters different $c_i$ we compute the mean distance between $x_i$ and all the points in the cluster. Then we take the cluster that has the mean distance as the closest cluster to $x_i$.\n",
    "\n",
    "We define $b_i$ as:\n",
    "\n",
    "${\\displaystyle b_i=\\min _{J\\neq I}{\\frac {1}{|C_{J}|}}\\sum _{j\\in C_{J}}d(i,j)}$\n",
    "\n",
    "With those $a_i$, and $b_I$ we define the silhouette score of the point $x_i$ as $s_i$ to be\n",
    "\n",
    "${\\displaystyle s_i={\\frac {b_i-a_i}{\\max\\{a_i,b_i\\}}}}$\n",
    "\n",
    "This value varies between -1 and 1. 1 means our clusters are dense, and -1 means the opposite.\n",
    "\n",
    "Let us write a Python function that will perform the clustering and return the k that gives us the best cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7d2ae-5dc2-40e8-a1d2-94f9961f78b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c83533-7b2d-43a8-8d08-3b0ee95bf6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_best_estimator(X):\n",
    "    \"\"\" compute the k mean clustering, and return the best k that maximizes the silhouette score\n",
    "    \"\"\"\n",
    "    k_mean_estimators = [\n",
    "        (f\"KMeans_{i}\", KMeans(n_clusters=i, random_state=42, max_iter=3000)) for i in range(3, X.shape[0])]\n",
    "    scores = []\n",
    "\n",
    "    best_estimator = None\n",
    "    best_metric = float(\"-inf\")\n",
    "    for estimator_name, estimator in k_mean_estimators:\n",
    "        estimator.fit(X)\n",
    "        labels = estimator. labels_\n",
    "        score = silhouette_score(\n",
    "            X, labels)\n",
    "        if score > best_metric:\n",
    "            best_metric = score\n",
    "            best_estimator = estimator\n",
    "        scores.append(score)\n",
    "    return best_estimator, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3b3a0",
   "metadata": {},
   "source": [
    "In the above function, we perform a clustering by selecting the number of clusters to be between 3 and the total number of documents, which is less than 100 in our case.\n",
    "\n",
    "The we compute the silhouette score, after computing the scores we return the estimator that gives us the best value of that score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9bd9c9-b3ef-40b6-a5c3-412fb4c62db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator, scores = find_best_estimator(today_news_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cbc7d-79a8-4327-9e0a-cadaae1de65b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Let plot now the similarity shilouette score and see how it grow with the number of cluster selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e21df-cfe4-4996-96c8-a90275fd7da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40dad8-fd90-43fd-b484-01597966e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.figure(figsize=(5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd789e45-e0cb-4089-95a0-a37afc9f5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.figure(figsize=(5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589eeacc-f9d2-4867-8fcd-89c096912657",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(3, today_news_embeddings.shape[0]), scores)\n",
    "ax.set_xlabel(\"Number of Clusters\")\n",
    "ax.set_ylabel(\"silhouette score\")\n",
    "ax.set_title(\"silhouette score vs Number of Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a496d41-a338-4d86-9825-0544fe901869",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34857d0b-ef3e-4652-a81b-491ab2180986",
   "metadata": {},
   "source": [
    "We can see that the best estimator gave us the n cluster equal to 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64053a4-29f2-48ca-b8f1-9a9a59f1eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"k_means_labels\"] = best_estimator.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dcbd6-c591-411b-98b9-32659bafec96",
   "metadata": {},
   "source": [
    "Now let us analyze the clustering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf776c-dbdd-404f-bd76-34b624b7bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_embeddings(dataframe, embeddings, index, label_column=\"labels\"):\n",
    "    \"\"\" take a matrix of embeddings and the labels.\n",
    "    for each label compute the cosine similarity of the document with that label.\n",
    "    \"\"\"\n",
    "    document_in_index = dataframe.query(f\"{label_column} == {index}\")\n",
    "    with pd.option_context('display.max_colwidth', None):\n",
    "        display(document_in_index.title)\n",
    "    document_index = document_in_index.index\n",
    "    vectors = embeddings[document_index]\n",
    "    return sentence_transformer_model.similarity(vectors,  vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f9f7a5-3a49-4524-8730-f7194c33c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_embeddings(news_df, today_news_embeddings,\n",
    "                   29, label_column=\"k_means_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d44eb2-da12-484d-9841-5f60f82601ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e4314-381f-4814-9847-1b661ce0476a",
   "metadata": {},
   "source": [
    "After the first look at the results we can see that the results are good, we have around 50 news clusters, for 79 news.\n",
    "Some news clusters have only one article which makes sense, and others have up to 6 articles. In the later analysis, we will only keep news articles that have more than one document.\n",
    "\n",
    "Can we do better than that? Let's now try hierarchical clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000cb32-4a9b-442f-8c9e-9e44449a7c02",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering is a clustering that uses an iterative approach to build the `dendrogram`.\n",
    "\n",
    "A dendrogram is a representation of a tree. Hierarchical clustering illustrates the arrangement of the clusters produced by the corresponding analyses.\n",
    "\n",
    "**How do we build a dendrogram?**\n",
    "\n",
    "Assuming we have `n` points that we would like to cluster, the algorithm starts with them and a similarity metric to use.\n",
    "In the first step, all the `n` points are grouped in a `n ` cluster, as each observation is treated as a separate cluster.\n",
    "Then, the next two similar clusters are fused into a cluster; at this point, we have `n-1` clusters.\n",
    "The algorithm will process iteratively by fusing similar clusters into each other until we have one cluster.  \n",
    "With one cluster we have our dendrogram complete.\n",
    "\n",
    "In the figure, [Put the figure here], illustrate a dendrogram resulting from the clustering.\n",
    "\n",
    "**How do we compute the similarity between clusters?**\n",
    "\n",
    "We have the notion of similarity between two points but how do we compute the similarity between a point and a cluster or between two clusters?\n",
    "The notion of similarity between two points can be extended to develop the notion of `linkage` which is how we evaluate the similarity between two groups of observation or clusters.\n",
    "Given two clusters A and B, linkage metrics start by computing the pairwise dissimilarity between the observations in cluster A and those in cluster B.\n",
    "\n",
    "Depending on how we compute the overall dissimilarity from those pairwise dissimilarities, the linkage metric will be defined.\n",
    "\n",
    "The linkage is called:\n",
    "\n",
    "- **complete**: When overall dissimilarity is the largest of the pairwise dissimilarity.\n",
    "\n",
    "- **single**: When overall dissimilarity is the smallest of the pairwise dissimilarity.\n",
    "\n",
    "- **average**: When overall dissimilarity is the average of the pairwise dissimilarity.\n",
    "\n",
    "With the understanding of Hierarchical clustering and the linkage metric, let's implement hierarchical clustering using the `scipy package`.\n",
    "\n",
    "### Hierarchical Clustering with Scipy.\n",
    "\n",
    "To implement the hierarchical clustering, we will use two functions from the `scipy` package, the `linkage` and the `dendrogram` function.\n",
    "\n",
    "The linkage function performs the clustering, it takes the input embeddings a numpy array, the linkage method, and the similarity metric and it returns the hierarchical clustering tree encoded as a linkage matrix.\n",
    "\n",
    "We use the `dendrogram` function to generate the tree plot of the linkage matrix.\n",
    "The bellow code illustrates how the clustering is performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e79d1c-b8e8-4674-b473-5351198aa30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a7622-a14c-4ce5-8b39-8e5a3597fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Linkage\n",
    "plt.figure(figsize=(20, 10))\n",
    "mergings = linkage(today_news_embeddings,\n",
    "                   method='complete', metric='cosine')\n",
    "dendrogram(mergings)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c86a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5827a512",
   "metadata": {},
   "source": [
    "As the result of the hierarchical clustering is a tree, which can be visualized as a dendrogram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478421e5",
   "metadata": {},
   "source": [
    "In the above plot, the x-axis represents the documents that are grouped into clusters based on the color, and the y-axis represents the distance cut-off used while computing the merging. The distance on the `y` axis is 1-cosine similarity.\n",
    "\n",
    "The _y_ axis represents the distance cut-off used while computing the merging.\n",
    "The _x_ axis represents the documents that are grouped into clusters based on the color.\n",
    "\n",
    "**A quick note on the `merging` value**:\n",
    "\n",
    "The result of the linkage function is a matrix, we will call it `merging matrix`.\n",
    "\n",
    "Each row of the merging matrix is in the format `[cluster_index, cluster_index, distance, sample_count]`, the column index is the ith iteration at which that merging was done.\n",
    "\n",
    "Recall that we said that hierarchical clustering considers each point as a separate cluster to start with and then iteratively merges those points two by two to create new clusters.\n",
    "\n",
    "Let's have a look at what happened in the first 8 iterations of our algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30141d9-984e-4435-985b-a1d64eb0ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merging_with_cosine_distance = mergings.copy()\n",
    "# the distance is 1 - the cosine similarity\n",
    "merging_with_cosine_distance[:, 2] = 1 - merging_with_cosine_distance[:, 2]\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(merging_with_cosine_distance[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d1bee",
   "metadata": {},
   "source": [
    "In the above matrix, we can see the first element contains `[9, 46, 0.92, 2]`. It tells us that after the first iteration the document with id 9 and 46 were grouped together and their cosine similarity is 0.92.\n",
    "\n",
    "Let see how what is inside those two documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(news_df.iloc[[9, 46]].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d62d",
   "metadata": {},
   "source": [
    "We can check other documents in the merging to understand how the merging works. When we get to the 9th iteration we can see that it merged 3 samples and the second cluster index at that iteration is not a single document, its index is 80 which is greater than the number of documents we have in our set(79). That means that cluster 50 has only one document: document 50 was merged with cluster 80 which contains 2 documents. They were merged at a distance of 0.878.\n",
    "\n",
    "The cluster 80 is built with documents merged in the `80 - len(document)th` iteration. In our case, it represents the documents merged in the 1st iteration(80 - 79). From the above matrix, we can see that in the first iteration documents 9 and 40 were merged. With that, we can say that at the 9th iteration, the three documents that were merged are documents with ID: 9, 46, and 50. Let's see what those documents look like from our set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fd0ab-bb7a-48b8-a0bd-8dc650e34e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8353454-7104-4ff6-860b-760d7e77e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(news_df.iloc[[9, 46, 50]].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1dfae-8398-4376-b6d6-dc0a6a74173f",
   "metadata": {},
   "source": [
    "By looking at the merging matrix we can understand how our clustering algorithm works! One of the main advantages of hierachical clustering is its explainability, in most business cases stakeholders will prefer to work with an explainable model than a non-explainable model or black box model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220ec26-a2ff-40e8-a32b-811340f664c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import fcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddfcf3-f1e7-473c-970e-bf5f177db94e",
   "metadata": {},
   "source": [
    "### Selecting the cluster labels for the documents.\n",
    "\n",
    "The `fcluster` function helps take a distance cut-off and return the cluster label of each document with the value of k as the distance cut-off. So if we say k = 0.2, the function will give us the clustering of the document assuming that the max distance of the document in a cluster is 0.2.\n",
    "\n",
    "But how do we find the best k to select?\n",
    "\n",
    "- We can use domain knowledge.\n",
    "- We can fine-tune that metric. We have decided to fine-tune, and by fine-tuning we select the k that gives us the best silhouette score.\n",
    "\n",
    "The below code performs fine-tuning and returns the k that maximizes the silhouette score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e2aaa-ce59-43d2-9bc7-270e30ba42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_distance(X, merging):\n",
    "    \"\"\" start with the document embedding x, and the hierarchical clustering, find the k that maximize the shilouette score\"\"\"\n",
    "    max_shilouette = float(\"-inf\")\n",
    "    return_labels = np.zeros(X.shape[0])\n",
    "    scores = []\n",
    "    number_of_clusters = []\n",
    "    best_k = 0\n",
    "    all_k = []\n",
    "    for k in np.arange(0.1, 0.5, 0.01):\n",
    "        labels = fcluster(merging, k, criterion=\"distance\")\n",
    "        n_clusters = np.unique(labels).shape[0]\n",
    "        if n_clusters <= 2 or n_clusters >= (X.shape[0]-1):\n",
    "            continue\n",
    "            # doesn't meet the condition\n",
    "        score = silhouette_score(\n",
    "            X, labels\n",
    "        )\n",
    "        scores.append(score)\n",
    "        all_k.append(k)\n",
    "        number_of_clusters.append(n_clusters)\n",
    "        if score > max_shilouette:\n",
    "            max_shilouette = score\n",
    "            return_labels = labels\n",
    "            best_k = k\n",
    "    return scores, return_labels, number_of_clusters, all_k, best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f77885-f273-4908-8794-8782eabdc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, label_hierarchical, number_of_clusters, all_k, best_k = select_best_distance(\n",
    "    today_news_embeddings, mergings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fa23e-1e29-40b6-8ddd-ad23b1e9afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db06ed2-7550-4517-bef2-4f93ae4b4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(all_k, scores)\n",
    "ax.set_xlabel(\"Distance metric\")\n",
    "ax.set_ylabel(\"silhouette score\")\n",
    "ax.set_title(\"silhouette score vs distance metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebaf11",
   "metadata": {},
   "source": [
    "The above plot illustrates how the silhouette score varies by the distance metric we select.\n",
    "Either at 0.4 stabilize at that point before a jump at around 0.4 to 0.65.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb55eab-4e0e-4062-ab76-0e95c876542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(label_hierarchical).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a74677-6270-4b2c-86ef-a89eeaaf10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f3a8c-1e73-4898-a37c-eecd5062f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bc649-85e1-4df2-ade7-6a54ab8d282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(all_k, number_of_clusters)\n",
    "ax.set_xlabel(\"Distance metric\")\n",
    "ax.set_ylabel(\"Number of clusters\")\n",
    "ax.set_title(\"distance vs number of clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25e47f-ccbc-4dc1-9b5e-0b3d9e5cd680",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb682fb-77dd-40b6-9e9d-6bb3f64afeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"label_hierachical\"] = label_hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257847f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"label_hierachical\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327419a3-4025-4102-a929-924b3ee120ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.query(\"label_hierachical == 11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5cde67-7aef-433b-823e-506439300a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_embeddings(news_df, today_news_embeddings, 4, \"label_hierachical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb946ea-d769-4931-a061-d3aa2d5c27f7",
   "metadata": {},
   "source": [
    "Once i have got the best labeling, i can go ahead and select the most important cluster.\n",
    "\n",
    "This will be all the cluster with more than 1 document, the rest of the document will be considered as noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c22c8-37a1-4a22-bb2b-06d08b7e576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = news_df.label_hierachical.value_counts()\n",
    "labels_with_more_than_one = cluster_counts[cluster_counts > 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff8db6-2aa8-41c9-a1f4-05c0ab031ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_news_df = news_df.loc[news_df.label_hierachical.isin(\n",
    "    labels_with_more_than_one)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(important_news_df.label_hierachical).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba61bc",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "In this notebook, we went through the process of building a new clustering system. We started by pulling the data from the database, then we computed the news embedding using the embedding model. With the embedding vectors of the news, we started the clustering. We explained the silhouette score, which is the metric we use to evaluate the quality of clusters resulting from a clustering algorithm, and then we explained and performed hierarchical clustering on our news embeddings. At the end of the hierarchical clustering, we ended up with news clusters finally we saved those data in a file for further analysis and downstream applications.\n",
    "\n",
    "In the next post, we will move from the jupyter notebook to a production-ready application. We will learn how to productionarize this simple news clustering system. Stay tuned for that post.\n",
    "\n",
    "At this point we have a notebook with the clustering results and those results are saved back in the folder. The next step will be to build a new cluster component that will be used in a downstream application.\n",
    "\n",
    "Reference:\n",
    "\n",
    "- [Article on The clustering](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)\n",
    "- Introduction to Statistical learning page 520-534.\n",
    "- Wikipedia on the clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c824ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

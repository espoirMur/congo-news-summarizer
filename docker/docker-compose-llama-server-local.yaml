services:
  llamacpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server
    environment:
      LLAMA_ARG_HF_REPO: espoir/congo_news_summarizer_qwen_models
      LLAMA_ARG_HF_FILE: Qwen_1.5_8Q.gguf
      LLAMA_ARG_CTX_SIZE: 4096
      LLAMA_ARG_N_PARALLEL: 1
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_PORT: 8000
      LLAMA_ARG_N_PREDICT: 512
    expose:
      - 8000
  nginx:
    depends_on: 
      - llamacpp-server
    build: 
      context: .
      dockerfile: Dockerfile-nginx
    ports: 
      - 80:80

services:
  llamacpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server
    volumes:
      - $PWD/models:/models
    environment:
      # alternatively, you can use "LLAMA_ARG_MODEL_URL" to download the model
      LLAMA_ARG_MODEL: /models/Qwen/Qwen_1.5_16Q.gguf  
      LLAMA_ARG_CTX_SIZE: 4096
      LLAMA_ARG_N_PARALLEL: 4
      LLAMA_ARG_ENDPOINT_METRICS: 1
      LLAMA_ARG_PORT: 8000
      LLAMA_ARG_N_PREDICT: 512
    expose:
      - 8000
  news-summarizer-generative-model:
    image: espymur/summarization-generator:latest
    pull_policy: always
    tty: true
    env_file:
      - ../.env_prod
    depends_on: 
      - llamacpp-server
    labels:
      ofelia.enabled: "true"
      ofelia.job-exec.generator.schedule:  "0 20 23 * * *"
      ofelia.job-exec.generator.command: "python src/llm/main.py -e prod"

    environment:
      - API_URL=http://llamacpp-server:8000
  nginx:
    depends_on: 
      - llamacpp-server
    build: 
      context: .
      dockerfile: Dockerfile-nginx
    ports: 
      - 80:80
  
  ofelia:
    image: mcuadros/ofelia:latest
    restart: "unless-stopped"
    depends_on:
      - news-summarizer-generative-model
    command: daemon --docker -f label=com.docker.compose.project=${COMPOSE_PROJECT_NAME}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro

